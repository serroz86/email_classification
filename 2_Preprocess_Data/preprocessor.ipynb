{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMART EMAIL CLASSIFIER\n",
    "\n",
    "## Preprocess Data\n",
    "The goal of this notebook is to preprocess the data.\n",
    "\n",
    "## 1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Uncomment the following line to install the nltk data\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Functions to clean and preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(original):\n",
    "# This function cleans a text by stopping, lemmatizing on verbs and excluding punctuations, email addresses, numbers, webpages and dates with \"/\"\n",
    "# It keeps only the nouns\n",
    "    lower = \" \".join([i for i in original.lower().split()])\n",
    "    no_fwd = lower.split('>from',1)[0].split('---',1)[0]    #removes forwarded emails and notes\n",
    "    cleantext = BeautifulSoup(no_fwd, \"lxml\").text\n",
    "    nouns = [word for word,pos in nltk.pos_tag(nltk.word_tokenize(' '.join([i for i in cleantext.split()]))) if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    lemmat = \" \".join(lemma.lemmatize(lemma.lemmatize(word),'v').translate({ord(ch): None for ch in '0123456789'}) for word in nouns.split()  if (\"@\" not in word and '.com' not in word and 'www' not in word and 'http' not in word and '/' not in word and ':' not in word))\n",
    "    normalized = re.sub('[%s]' % re.escape(string.punctuation), ' ', lemmat).split()\n",
    "    x= [s for s in normalized if s not in stop]\n",
    "    y = [s for s in x if (len(s) > 2 and len(s) < 15) ]\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(infile,outfile,n_sample,restriction,restriction_by_area):\n",
    "    df=pd.read_csv(infile).fillna(value=0)\n",
    "    if n_sample == None:\n",
    "        n_sample=len(df)\n",
    "\n",
    "    if restriction == True:\n",
    "        # To restrict the emails to those 1-1 emails sent to a different person, and are not fwd or reply emails\n",
    "        df['senders_num']=df['to'].str.count('@').fillna(value=0)\n",
    "        df_no_fwd=df.loc[~df['subject'].str.contains('fwd',case=False,na=False)]\n",
    "        df_one_to_one=df_no_fwd.loc[df_no_fwd['senders_num']==1]\n",
    "        df_one_to_one.to_csv('../data/ingest/all/1-1.csv',index_label=False)      \n",
    "        df_no_re=df_one_to_one.loc[~df_one_to_one['subject'].str.contains('re',case=False,na=False)]\n",
    "        df_restricted=df_no_re.loc[df_no_re['to']!=df_no_re['from']].reset_index(drop=True)      \n",
    "        df_restricted.to_csv('../data/ingest/all/subset.csv',index_label=False)      \n",
    "        df_sample=df_restricted.sample(n=n_sample)\n",
    "        if restriction_by_area == True:\n",
    "            df_enron=pd.read_csv('../data/employees.csv').fillna(value=0)\n",
    "            df_areas=df_enron.loc[df_enron['area']!=0]\n",
    "            df1=df_areas[['email1','area']]\n",
    "            df2=df_areas[['email2','area']]\n",
    "            df3=df_areas[['email3','area']]\n",
    "            df4=df_areas[['email4','area']]\n",
    "            df1_comb=df_restricted.merge( df1, how='inner', left_on=['from'], right_on=['email1']).drop(columns=['email1'])\n",
    "            df2_comb=df_restricted.merge( df2, how='inner', left_on=['from'], right_on=['email2']).drop(columns=['email2'])\n",
    "            df3_comb=df_restricted.merge( df3, how='inner', left_on=['from'], right_on=['email3']).drop(columns=['email3'])\n",
    "            df4_comb=df_restricted.merge( df4, how='inner', left_on=['from'], right_on=['email4']).drop(columns=['email4'])\n",
    "            df1_comb2=df_restricted.merge( df1, how='inner', left_on=['to'], right_on=['email1']).drop(columns=['email1'])\n",
    "            df2_comb2=df_restricted.merge( df2, how='inner', left_on=['to'], right_on=['email2']).drop(columns=['email2'])\n",
    "            df3_comb2=df_restricted.merge( df3, how='inner', left_on=['to'], right_on=['email3']).drop(columns=['email3'])\n",
    "            df4_comb2=df_restricted.merge( df4, how='inner', left_on=['to'], right_on=['email4']).drop(columns=['email4'])\n",
    "            df_comb=df1_comb.append(df2_comb).append(df3_comb).append(df4_comb).append(df1_comb2).append(df2_comb2).append(df3_comb2).append(df4_comb2)\n",
    "            \n",
    "            \n",
    "    else:     \n",
    "        df_sample=df.sample(n=n_sample)\n",
    "    if restriction_by_area == True:\n",
    "        for area in df_comb['area'].unique():\n",
    "            print(area, len(df_comb[df_comb['area']==area]))\n",
    "            emails_clean = [clean(email) for email in df_comb['message'][df_comb['area'] == area].tolist()]\n",
    "            with open('../data/preprocessed/preprocessed_%s.csv'%area, 'w', newline='') as g:\n",
    "                for email in emails_clean:\n",
    "                    if len(email) > 0:                    \n",
    "                        g.write(\" \".join(email)+'\\n')\n",
    "    else:     \n",
    "        emails_clean = [clean(email) for email in df_sample['message'].tolist()]\n",
    "        # We save the clean text in a csv file without a dataframe structure, as it is much faster to read/write\n",
    "        with open(outfile, 'w', newline='') as g:\n",
    "            for email in emails_clean:\n",
    "                if len(email) > 0:\n",
    "                    g.write(\" \".join(email)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('../data/preprocessed')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # We define the following variables for the preprocessing\n",
    "    # We remove the most common words in the English dictionary\n",
    "    stoplist = ['also use make people know many call include part find become like mean often different usually take with come give well get since type list say change see refer actually iii kinds ask would way something need things want every str =09 0909 image'.split(' ')][0]\n",
    "    stop = set(list(stopwords.words('english'))+stoplist)\n",
    "    # We will also exclude the punctuation signs\n",
    "    exclude = set(string.punctuation) \n",
    "    # We will also lemmatize\n",
    "    lemma = WordNetLemmatizer()\n",
    "       \n",
    "    infile = '../data/ingest/all/emails.csv'\n",
    "    outfile = \"../data/preprocessed/preprocessed_pos.csv\"\n",
    "    n_sample = 70000     #number of random emails to preprocess, select a number if only a sample is desired\n",
    "#     n_sample = None    #if this line is uncommented, all the emails would be processed\n",
    "    restriction=True     # set True if we want to restrict the analysis to 1-1 emails that are not fwd or reply\n",
    "    restriction_by_area=False   # set True if the 1-1 emails are restricted to ENRON emails and processed by area\n",
    "    \n",
    "    preprocessing(infile,outfile,n_sample,restriction,restriction_by_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
